{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Deep Q-Network implementation.\n",
    "\n",
    "PyTorch debug mode\n",
    "\n",
    "This homework shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way.\n",
    "\n",
    "Original paper:\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is given for debug.** The main DQN task will be in the same folder later. The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
    "\n",
    "**We suggest the following pipeline:** First implement debug notebook then implement the main one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# prefix = 'https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring19/week04_approx_rl/'\n",
    "\n",
    "# os.system('wget ' + prefix + 'atari_wrappers.py')\n",
    "# os.system('wget ' + prefix + 'utils.py')\n",
    "# os.system('wget ' + prefix + 'replay_buffer.py')\n",
    "# os.system('wget ' + prefix + 'framebuffer.py')\n",
    "\n",
    "# print('setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for pytoch, but you find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from IPython.display import Video, clear_output\n",
    "\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "#ENV_NAME = 'MountainCar-v0'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # CartPole is wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n\n",
    "print(state_shape)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Description:\n",
      "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
      "        a frictionless track. The pendulum starts upright, and the goal is to\n",
      "        prevent it from falling over by increasing and reducing the cart's\n",
      "        velocity.\n",
      "\n",
      "    Source:\n",
      "        This environment corresponds to the version of the cart-pole problem\n",
      "        described by Barto, Sutton, and Anderson\n",
      "\n",
      "    Observation:\n",
      "        Type: Box(4)\n",
      "        Num     Observation               Min                     Max\n",
      "        0       Cart Position             -4.8                    4.8\n",
      "        1       Cart Velocity             -Inf                    Inf\n",
      "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
      "        3       Pole Angular Velocity     -Inf                    Inf\n",
      "\n",
      "    Actions:\n",
      "        Type: Discrete(2)\n",
      "        Num   Action\n",
      "        0     Push cart to the left\n",
      "        1     Push cart to the right\n",
      "\n",
      "        Note: The amount the velocity that is reduced or increased is not\n",
      "        fixed; it depends on the angle the pole is pointing. This is because\n",
      "        the center of gravity of the pole increases the amount of energy needed\n",
      "        to move the cart underneath it\n",
      "\n",
      "    Reward:\n",
      "        Reward is 1 for every step taken, including the termination step\n",
      "\n",
      "    Starting State:\n",
      "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
      "\n",
      "    Episode Termination:\n",
      "        Pole Angle is more than 12 degrees.\n",
      "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
      "        the display).\n",
      "        Episode length is greater than 200.\n",
      "        Solved Requirements:\n",
      "        Considered solved when the average return is greater than or equal to\n",
      "        195.0 over 100 consecutive trials.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play yourself"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(200):\n",
    "    action = int(input())\n",
    "    env.step(action)\n",
    "    plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sleep = 0.05\n",
    "\n",
    "if not hasattr(env.action_space, 'n'):\n",
    "    raise Exception('Keyboard agent only supports discrete action spaces')\n",
    "ACTIONS = env.action_space.n\n",
    "SKIP_CONTROL = 0    # Use previous control decision SKIP_CONTROL times, that's how you\n",
    "                    # can test what skip is still usable.\n",
    "\n",
    "human_agent_action = 0\n",
    "human_wants_restart = False\n",
    "human_sets_pause = False\n",
    "\n",
    "def key_press(key, mod):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    if key==0xff0d: human_wants_restart = True\n",
    "    if key==32: human_sets_pause = not human_sets_pause\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    human_agent_action = a\n",
    "\n",
    "def key_release(key, mod):\n",
    "    global human_agent_action\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    if human_agent_action == a:\n",
    "        human_agent_action = 0\n",
    "\n",
    "env.render()\n",
    "env.unwrapped.viewer.window.on_key_press = key_press\n",
    "env.unwrapped.viewer.window.on_key_release = key_release\n",
    "\n",
    "def rollout(env):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    human_wants_restart = False\n",
    "    obser = env.reset()\n",
    "    skip = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "    while 1:\n",
    "        if not skip:\n",
    "            #print(\"taking action {}\".format(human_agent_action))\n",
    "            a = human_agent_action\n",
    "            total_timesteps += 1\n",
    "            skip = SKIP_CONTROL\n",
    "        else:\n",
    "            skip -= 1\n",
    "\n",
    "        obser, r, done, info = env.step(a)\n",
    "        if r != 0:\n",
    "            print(\"reward %0.3f\" % r)\n",
    "        total_reward += r\n",
    "        window_still_open = env.render()\n",
    "        if window_still_open==False: return False\n",
    "        if done: break\n",
    "        if human_wants_restart: break\n",
    "        while human_sets_pause:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        time.sleep(sleep)\n",
    "    print(\"timesteps %i reward %0.2f\" % (total_timesteps, total_reward))\n",
    "\n",
    "print(\"ACTIONS={}\".format(ACTIONS))\n",
    "print(\"Press keys 1 2 3 ... to take actions 1 2 3 ...\")\n",
    "print(\"No keys pressed is taking action 0\")\n",
    "\n",
    "while 1:\n",
    "    window_still_open = rollout(env)\n",
    "    if window_still_open==False: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_view_window():\n",
    "    from gym.envs.classic_control import rendering\n",
    "    org_constructor = rendering.Viewer.__init__\n",
    "\n",
    "    def constructor(self, *args, **kwargs):\n",
    "        org_constructor(self, *args, **kwargs)\n",
    "        self.window.set_visible(visible=False)\n",
    "\n",
    "    rendering.Viewer.__init__ = constructor\n",
    "disable_view_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_run(video_folder, rewards, idx=None, best=False, worse=False):\n",
    "\n",
    "    assert idx == None or (best == False and worse == False), \"both idx and best / worse are specified\"\n",
    "    assert best == False or worse == False, \"both flags are up\"\n",
    "    \n",
    "    runs = natural_sort(glob.glob(video_folder + \"*.mp4\"))\n",
    "    \n",
    "    assert len(runs) == len(rewards), f\"different number of files ({len(runs)})  and rewards ({len(rewards)})\"\n",
    "    \n",
    "    if best:\n",
    "        idx = np.argmax(rewards)\n",
    "    elif worse:\n",
    "        idx = np.argmin(rewards)\n",
    "    elif idx == None:\n",
    "        idx = random.randint(0, len(runs) - 1)\n",
    "        \n",
    "    run = runs[idx]\n",
    "    reward = rewards[idx]\n",
    "    \n",
    "    print(f\"displaying '{run}' run\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    display(Video(run, html_attributes=\"controls loop autoplay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, policy, max_steps=np.inf, render=False, sleep=0.01, video_folder=None):\n",
    "    \n",
    "    if video_folder is not None:\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
    "    \n",
    "    state = env.reset()\n",
    "    iter_reward = 0\n",
    "    cur_step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and cur_step < max_steps:\n",
    "        \n",
    "        action = policy.get_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        iter_reward += reward\n",
    "        cur_step += 1\n",
    "        \n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(env.render(\"rgb_array\"))\n",
    "            plt.show()\n",
    "\n",
    "    return iter_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(env, policy, n_iter=200, max_steps=np.inf, video_folder=None):\n",
    "\n",
    "    rewards = np.zeros(n_iter)\n",
    "    if video_folder is not None:\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
    "    \n",
    "    for i in tqdm(range(n_iter)):\n",
    "        \n",
    "        rewards[i] = play_game(env, policy, max_steps=max_steps)\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_policy(env, policy, n_iter=200, n_iter_record=30, max_steps=np.inf, video_folder=\"./runs/\"):\n",
    "\n",
    "    rewards = eval_policy(env, policy, n_iter=n_iter, max_steps=max_steps)\n",
    "    \n",
    "    sns.boxplot(x=rewards)\n",
    "    plt.title(\"reward distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    video_folder = video_folder + type(policy).__name__ + '/'\n",
    "    rewards = eval_policy(env, policy, n_iter=n_iter_record, max_steps=max_steps, video_folder=video_folder)\n",
    "    \n",
    "    print(f\"Best run (out of {n_iter_record} sampled):\")\n",
    "    show_run(video_folder, rewards, best=True)\n",
    "    \n",
    "    return video_folder, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(env)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "play_game(env, random_policy, max_steps=200, render=True, video_folder=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 4240.12it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run (out of 30 sampled):\n",
      "displaying './runs/RandomPolicy/rl-video-episode-19.mp4' run\n",
      "reward: 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./runs/RandomPolicy/rl-video-episode-19.mp4\" controls loop autoplay  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_policy = RandomPolicy(env)\n",
    "video_folder, rewards = analyze_policy(env, random_policy, max_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnglePolicy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        angle = state[2]\n",
    "        return 1 if angle > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2603.23it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run (out of 30 sampled):\n",
      "displaying './runs/AnglePolicy/rl-video-episode-20.mp4' run\n",
      "reward: 68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./runs/AnglePolicy/rl-video-episode-20.mp4\" controls loop autoplay  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "angel_policy = AnglePolicy()\n",
    "video_folder, rewards = analyze_policy(env, angel_policy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_run(video_folder, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01108899, -0.02510765, -0.03262883,  0.04199962])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.07\n"
     ]
    }
   ],
   "source": [
    "render = False\n",
    "sleep = 0.05\n",
    "rewards = np.zeros(n_iter)\n",
    "data = np.ndarray(n_iter, dtype=object)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    env.reset()\n",
    "    reward_local = 0\n",
    "    policy = RandomPolicy()\n",
    "    local_data = []\n",
    "    new_state = env.state\n",
    "    \n",
    "    for step in range(200):\n",
    "        \n",
    "        #action = policy.get_action()\n",
    "        action = model.predict([new_state])[0]\n",
    "        local_data.append((new_state, action))\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        reward_local += reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards[i] = reward_local\n",
    "    data[i] = local_data\n",
    "    \n",
    "print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([181., 195., 193., 200., 174., 135., 157., 180., 200., 200., 200.,\n",
       "       156., 131., 200., 190., 200., 119., 200., 192., 200., 200., 200.,\n",
       "       139., 161., 198., 200., 199., 189., 200., 142., 176., 200., 200.,\n",
       "       200., 200., 200., 200., 200., 200., 184., 200., 131., 138., 191.,\n",
       "       166., 200., 200., 200., 200., 187., 163., 200., 200., 200., 153.,\n",
       "       200., 200., 200., 200., 180., 200., 136., 200., 200., 200., 187.,\n",
       "       156., 200., 200., 200., 200., 200., 141., 200., 185., 200., 200.,\n",
       "       200., 173., 200., 152., 178., 200., 200., 196., 200., 157., 135.,\n",
       "       200., 182., 200., 187., 200., 137., 181., 200., 200., 179., 178.,\n",
       "       167.])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_of_top_runs = np.argsort(rewards)[::-1][:int(len(rewards)*(1-percentile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furiousteabag/.local/share/virtualenvs/base/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.partial_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for run in data[indexes_of_top_runs]:\n",
    "    for state, action in run:\n",
    "        X.append(state)\n",
    "        y.append(action)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04106762, -0.00543333, -0.03818392, -0.04059633],\n",
       "       [ 0.04095895, -0.19998751, -0.03899585,  0.23979889],\n",
       "       [ 0.0369592 , -0.00433082, -0.03419987, -0.06492491],\n",
       "       ...,\n",
       "       [ 0.21527827, -0.56317478, -0.02783623,  0.27241112],\n",
       "       [ 0.20401477, -0.7578887 , -0.022388  ,  0.55618606],\n",
       "       [ 0.188857  , -0.56245971, -0.01126428,  0.25653451]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyPolicy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = MLPRegressor()\n",
    "    \n",
    "    def get_action(self):\n",
    "        return [0, 1][random() > 0.5]\n",
    "    \n",
    "    def update_policy(self, new_state, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        <YOUR CODE>\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = <YOUR CODE>\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, n_games=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in previous bonus assigments, you can copy-paste it here in main notebook.\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
    "                   1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    <YOUR CODE >\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
    "                                 \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n",
    "                                    \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
    "                                        np.mean(is_dones), len(exp_replay))\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done should be strictly True or False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions]\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "\n",
    "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
    "\n",
    "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(\n",
    "        len(actions)), actions]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = <YOUR CODE>\n",
    "\n",
    "    assert next_state_values.dim(\n",
    "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
    "    ), \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = <your favourite random seed>\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10**4)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for step in trange(total_steps + 1):\n",
    "    if not utils.is_enough_ram():\n",
    "        print('less that 100 Mb RAM available, freezing')\n",
    "        print('make sure everything is ok and make KeyboardInterrupt to continue')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "    # play\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "    # train\n",
    "    < sample batch_size of data from experience replay >\n",
    "\n",
    "    loss = < compute TD loss >\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        <YOUR CODE >\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        # eval the agent\n",
    "        mean_rw_history.append(evaluate(\n",
    "            make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "        )\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [make_env(seed=step).reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward per episode\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Initial state V\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "assert final_score > 300, 'not good enough for DQN'\n",
    "print('Well done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
